{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6322,"databundleVersionId":868312,"sourceType":"competition"},{"sourceId":938046,"sourceType":"datasetVersion","datasetId":503255}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-30T11:43:49.931230Z","iopub.execute_input":"2024-10-30T11:43:49.931653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom itertools import chain","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:04:37.983640Z","iopub.execute_input":"2024-10-30T17:04:37.985119Z","iopub.status.idle":"2024-10-30T17:04:55.182386Z","shell.execute_reply.started":"2024-10-30T17:04:37.985030Z","shell.execute_reply":"2024-10-30T17:04:55.180938Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"labels_df = pd.read_csv('/kaggle/input/planets-dataset/planet/planet/train_classes.csv')\nlabels_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:47:26.981609Z","iopub.execute_input":"2024-10-30T11:47:26.982031Z","iopub.status.idle":"2024-10-30T11:47:27.059525Z","shell.execute_reply.started":"2024-10-30T11:47:26.981993Z","shell.execute_reply":"2024-10-30T11:47:27.058342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print all unique tags\nfrom itertools import chain\nlabels_list = list(chain.from_iterable([tags.split(\" \") for tags in labels_df['tags'].values]))\nlabels_set = set(labels_list)\nprint(\"There is {} unique labels including {}\".format(len(labels_set), labels_set))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:47:47.994423Z","iopub.execute_input":"2024-10-30T11:47:47.994861Z","iopub.status.idle":"2024-10-30T11:47:48.043028Z","shell.execute_reply.started":"2024-10-30T11:47:47.994821Z","shell.execute_reply":"2024-10-30T11:47:48.041648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of label instances\nlabels_s = pd.Series(labels_list).value_counts() # To sort them by count\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x=labels_s, y=labels_s.index, orient='h')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:48:53.339002Z","iopub.execute_input":"2024-10-30T11:48:53.339411Z","iopub.status.idle":"2024-10-30T11:48:54.077587Z","shell.execute_reply.started":"2024-10-30T11:48:53.339376Z","shell.execute_reply":"2024-10-30T11:48:54.075969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_title = [labels_df[labels_df['tags'].str.contains(label)].iloc[i]['image_name'] + '.jpg' \n                for i, label in enumerate(labels_set)]\n\nplt.rc('axes', grid=False)\n_, axs = plt.subplots(5, 4, sharex='col', sharey='row', figsize=(15, 20))\naxs = axs.ravel()\n\nfor i, (image_name, label) in enumerate(zip(images_title, labels_set)):\n    img = mpimg.imread('/kaggle/input/planets-dataset/planet/planet/train-jpg' + '/' + image_name)\n    axs[i].imshow(img)\n    axs[i].set_title('{} - {}'.format(image_name, label))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:49:41.205779Z","iopub.execute_input":"2024-10-30T11:49:41.206216Z","iopub.status.idle":"2024-10-30T11:49:46.841065Z","shell.execute_reply.started":"2024-10-30T11:49:41.206178Z","shell.execute_reply":"2024-10-30T11:49:46.839449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input parameters\ntrain_jpeg_dir = '/kaggle/input/planets-dataset/planet/planet/train-jpg'\ntrain_csv_file = '/kaggle/input/planets-dataset/planet/planet/train_classes.csv'\ntest_jpeg_dir = '/kaggle/input/planets-dataset/planet/planet/test-jpg'\ntest_additional_jpeg_dir = '/kaggle/input/planets-dataset/test-jpg-additional/test-jpg-additional'\nimg_resize = (128, 128)  # Desired image size\nvalidation_split = 0.2\nbatch_size = 32\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:06:46.917646Z","iopub.execute_input":"2024-10-30T17:06:46.918125Z","iopub.status.idle":"2024-10-30T17:06:46.925098Z","shell.execute_reply.started":"2024-10-30T17:06:46.918081Z","shell.execute_reply":"2024-10-30T17:06:46.923544Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Read the CSV file\nlabels_df = pd.read_csv(train_csv_file)\n\n# Extract all unique labels\nlabels = sorted(set(chain.from_iterable([tags.split(\" \") for tags in labels_df['tags'].values])))\n\n# Create a mapping from label to index\nlabels_map = {label: idx for idx, label in enumerate(labels)}\nnum_classes = len(labels_map)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:06:48.619242Z","iopub.execute_input":"2024-10-30T17:06:48.619761Z","iopub.status.idle":"2024-10-30T17:06:48.715319Z","shell.execute_reply.started":"2024-10-30T17:06:48.619711Z","shell.execute_reply":"2024-10-30T17:06:48.713875Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Function to one-hot encode the tags\ndef encode_tags(tags_str):\n    tags = tags_str.split(' ')\n    targets = np.zeros(num_classes, dtype='float32')\n    for tag in tags:\n        targets[labels_map[tag]] = 1.0\n    return targets\n\n# Apply the encoding to the DataFrame\nlabels_df['targets'] = labels_df['tags'].apply(encode_tags)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:06:50.248185Z","iopub.execute_input":"2024-10-30T17:06:50.248666Z","iopub.status.idle":"2024-10-30T17:06:50.376978Z","shell.execute_reply.started":"2024-10-30T17:06:50.248612Z","shell.execute_reply":"2024-10-30T17:06:50.375576Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Split the DataFrame into training and validation sets\ntrain_df, val_df = train_test_split(labels_df, test_size=validation_split, random_state=42)\n\n# Reset indices\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:06:51.707791Z","iopub.execute_input":"2024-10-30T17:06:51.708245Z","iopub.status.idle":"2024-10-30T17:06:51.758634Z","shell.execute_reply.started":"2024-10-30T17:06:51.708202Z","shell.execute_reply":"2024-10-30T17:06:51.757284Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Function to load and preprocess images\ndef load_and_preprocess_image(path, label):\n    # Read the image file\n    image = tf.io.read_file(path)\n    # Decode the image\n    image = tf.image.decode_jpeg(image, channels=3)\n    # Resize the image\n    image = tf.image.resize(image, img_resize)\n    # Normalize the image to [-1, 1]\n    image = (image / 127.5) - 1.0\n    return image, label\n\n# Function to create a dataset from the DataFrame\ndef create_dataset(df, training=True):\n    # Construct full file paths\n    image_paths = df['image_name'].apply(lambda x: os.path.join(train_jpeg_dir, f\"{x}.jpg\")).tolist()\n    labels = np.stack(df['targets'].values)\n    # Create a TensorFlow Dataset\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    # Map the load_and_preprocess_image function to the dataset\n    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n    if training:\n        # Shuffle and repeat for training\n        dataset = dataset.shuffle(buffer_size=1000)\n    # Batch and prefetch the dataset\n    dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:06:54.338301Z","iopub.execute_input":"2024-10-30T17:06:54.338789Z","iopub.status.idle":"2024-10-30T17:06:54.350056Z","shell.execute_reply.started":"2024-10-30T17:06:54.338741Z","shell.execute_reply":"2024-10-30T17:06:54.348492Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Create the training dataset\ntrain_dataset = create_dataset(train_df, training=True)\n\n# Create the validation dataset\nval_dataset = create_dataset(val_df, training=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:06:56.790043Z","iopub.execute_input":"2024-10-30T17:06:56.790469Z","iopub.status.idle":"2024-10-30T17:06:57.199463Z","shell.execute_reply.started":"2024-10-30T17:06:56.790431Z","shell.execute_reply":"2024-10-30T17:06:57.198238Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Get test file paths\ntest_files = [os.path.join(test_jpeg_dir, f) for f in os.listdir(test_jpeg_dir)]\ntest_files += [os.path.join(test_additional_jpeg_dir, f) for f in os.listdir(test_additional_jpeg_dir)]\n\n# Function to load and preprocess test images\ndef load_and_preprocess_test_image(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, img_resize)\n    image = (image / 127.5) - 1.0\n    return image\n\n# Create test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_files)\ntest_dataset = test_dataset.map(load_and_preprocess_test_image, num_parallel_calls=AUTOTUNE)\ntest_dataset = test_dataset.batch(batch_size).prefetch(AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:06:58.628143Z","iopub.execute_input":"2024-10-30T17:06:58.628605Z","iopub.status.idle":"2024-10-30T17:06:59.129255Z","shell.execute_reply.started":"2024-10-30T17:06:58.628540Z","shell.execute_reply":"2024-10-30T17:06:59.127984Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Example: Display shapes of batches\nfor images, labels in train_dataset.take(1):\n    print(images.shape)  # Expected: (batch_size, img_resize[0], img_resize[1], 3)\n    print(labels.shape)  # Expected: (batch_size, num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:07:00.421479Z","iopub.execute_input":"2024-10-30T17:07:00.421998Z","iopub.status.idle":"2024-10-30T17:07:01.564936Z","shell.execute_reply.started":"2024-10-30T17:07:00.421945Z","shell.execute_reply":"2024-10-30T17:07:01.563678Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(32, 128, 128, 3)\n(32, 17)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-30T17:08:52.607980Z","iopub.execute_input":"2024-10-30T17:08:52.608456Z","iopub.status.idle":"2024-10-30T17:08:52.617455Z","shell.execute_reply.started":"2024-10-30T17:08:52.608411Z","shell.execute_reply":"2024-10-30T17:08:52.615668Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 17), dtype=tf.float32, name=None))>"},"metadata":{}}]}]}